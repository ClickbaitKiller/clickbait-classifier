{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pos = pd.read_table('clickbait_data', header=None, names=['X'])\n",
    "df_neg = pd.read_table('non_clickbait_data', header=None, names=['X'])\n",
    "\n",
    "\n",
    "# add some ad-hoc data\n",
    "df_neg = df_neg.append([{'X': '<3'}]*50, ignore_index = True)\n",
    "df_neg = df_neg.append([{'X': 'connexion login join rejoignez'}]*50, ignore_index = True)\n",
    "\n",
    "\n",
    "# combine\n",
    "df_pos['Y'] = True\n",
    "df_neg['Y'] = False\n",
    "\n",
    "df = pd.concat([df_pos, df_neg], ignore_index = True)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r'\\d')\n",
    "# df_neg.apply(lambda row: len(regex.search(row['X'])) > 0, axis=0)\n",
    "\n",
    "def f(row):\n",
    "    return row[1] and regex.search(row[0]) != None\n",
    "\n",
    "df['list'] = df.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bill Changing Credit Card Rules Is Sent to Oba...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Hollywood, the Easy-Money Generation Toughe...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1700 runners still unaccounted for in UK's Lak...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yankees Pitchers Trade Fielding Drills for Put...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Large earthquake rattles Indonesia; Seventh in...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Coldplay's new album hits stores worldwide thi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>U.N. Leader Presses Sri Lanka on Speeding Reli...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2 Somali-Americans Charged With Aiding Terror</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US Highway Administration releases interim rep...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>White House Announces International Meetings t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>With Troubled Coyotes, Gretzky Called On as Sa...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Door opens mid-Qantas flight; plane makes an e...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gas prices surge in Northeast US</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Schapelle Corby found guilty, sentenced to 20 ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008 SecuTech Expo starts in Taipei, Taiwan fo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>New year introduces Illinois texting while dri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Rod Woodson Tries to Stay Grounded Amid Hall o...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Former US President Gerald Ford hospitalized w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Obama Welcomes Specter to the Party</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>New law to help asbestos sufferers in Victoria...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>France Approves Crackdown on Internet Piracy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>With Fine Print, the Rollout Dazzles This Time</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>UK's most-read papers found to be in contempt ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>As Germany Sinks, Its Economic System Is Quest...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tropical Storm Dolores now active</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Car Bomb Kills Police Official in Spain</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>'Invitational Games for the Deaf, Taipei 2008'...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>US government stops Haiti evacuations</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Credit Card Industry Aims to Profit From Sterl...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Google Maps incorporates satellite images</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16021</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16022</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16023</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16024</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16025</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16026</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16027</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16028</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16029</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16030</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16031</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16032</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16033</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16034</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16035</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16036</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16037</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16038</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16039</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16040</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16041</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16042</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16043</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16044</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16045</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16046</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16047</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16048</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16049</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16050</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16051 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       X      Y\n",
       "0      Bill Changing Credit Card Rules Is Sent to Oba...  False\n",
       "1      In Hollywood, the Easy-Money Generation Toughe...  False\n",
       "2      1700 runners still unaccounted for in UK's Lak...  False\n",
       "3      Yankees Pitchers Trade Fielding Drills for Put...  False\n",
       "4      Large earthquake rattles Indonesia; Seventh in...  False\n",
       "5      Coldplay's new album hits stores worldwide thi...  False\n",
       "6      U.N. Leader Presses Sri Lanka on Speeding Reli...  False\n",
       "7          2 Somali-Americans Charged With Aiding Terror  False\n",
       "8      US Highway Administration releases interim rep...  False\n",
       "9      White House Announces International Meetings t...  False\n",
       "10     With Troubled Coyotes, Gretzky Called On as Sa...  False\n",
       "11     Door opens mid-Qantas flight; plane makes an e...  False\n",
       "12                      Gas prices surge in Northeast US  False\n",
       "13     Schapelle Corby found guilty, sentenced to 20 ...  False\n",
       "14     2008 SecuTech Expo starts in Taipei, Taiwan fo...  False\n",
       "15     New year introduces Illinois texting while dri...  False\n",
       "16     Rod Woodson Tries to Stay Grounded Amid Hall o...  False\n",
       "17     Former US President Gerald Ford hospitalized w...  False\n",
       "18                   Obama Welcomes Specter to the Party  False\n",
       "19     New law to help asbestos sufferers in Victoria...  False\n",
       "20          France Approves Crackdown on Internet Piracy  False\n",
       "21        With Fine Print, the Rollout Dazzles This Time  False\n",
       "22     UK's most-read papers found to be in contempt ...  False\n",
       "23     As Germany Sinks, Its Economic System Is Quest...  False\n",
       "24                     Tropical Storm Dolores now active  False\n",
       "25               Car Bomb Kills Police Official in Spain  False\n",
       "26     'Invitational Games for the Deaf, Taipei 2008'...  False\n",
       "27                 US government stops Haiti evacuations  False\n",
       "28     Credit Card Industry Aims to Profit From Sterl...  False\n",
       "29             Google Maps incorporates satellite images  False\n",
       "...                                                  ...    ...\n",
       "16021                                                 <3  False\n",
       "16022                                                 <3  False\n",
       "16023                                                 <3  False\n",
       "16024                                                 <3  False\n",
       "16025                                                 <3  False\n",
       "16026                                                 <3  False\n",
       "16027                                                 <3  False\n",
       "16028                                                 <3  False\n",
       "16029                                                 <3  False\n",
       "16030                                                 <3  False\n",
       "16031                                                 <3  False\n",
       "16032                                                 <3  False\n",
       "16033                                                 <3  False\n",
       "16034                                                 <3  False\n",
       "16035                                                 <3  False\n",
       "16036                                                 <3  False\n",
       "16037                                                 <3  False\n",
       "16038                                                 <3  False\n",
       "16039                                                 <3  False\n",
       "16040                                                 <3  False\n",
       "16041                                                 <3  False\n",
       "16042                                                 <3  False\n",
       "16043                                                 <3  False\n",
       "16044                                                 <3  False\n",
       "16045                                                 <3  False\n",
       "16046                                                 <3  False\n",
       "16047                                                 <3  False\n",
       "16048                                                 <3  False\n",
       "16049                                                 <3  False\n",
       "16050                                                 <3  False\n",
       "\n",
       "[16051 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc': 0.99124261187728679, 'precision': 0.96206896551724141, 'recall': 0.95015479876160991, 'f1': 0.95607476635514022}\n",
      "{'roc': 0.99035281978560175, 'precision': 0.96636771300448432, 'recall': 0.93958268452195581, 'f1': 0.95278698878888368}\n",
      "{'roc': 0.99051836524282266, 'precision': 0.96524590163934432, 'recall': 0.94571153228397042, 'f1': 0.95537887392503651}\n",
      "{'roc': 0.991301684532925, 'precision': 0.96606851549755302, 'recall': 0.93851030110935019, 'f1': 0.95209003215434074}\n",
      "{'roc': 0.99022247686376563, 'precision': 0.9684276336355111, 'recall': 0.94164133738601818, 'f1': 0.95484666358452763}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "def gen_scores(y_true, y_pred, y_pred_proba):\n",
    "    roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return { 'roc': roc, 'precision': precision, 'recall': recall, 'f1': f1 }\n",
    "   \n",
    "def process_features(sentence):\n",
    "#     return re.sub(r'\\d+', '0', sentence)\n",
    "    return sentence\n",
    "#     ret = \"\"\n",
    "    \n",
    "#     for w in sentence:\n",
    "#         if w.isdigit() and int(w) < 110:\n",
    "#             ret += '0'\n",
    "#         else:\n",
    "#             ret += w\n",
    "            \n",
    "#     return ret\n",
    "    \n",
    "size = 30\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, test in kf.split(df):\n",
    "    X_train, X_test, y_train, y_test = df['X'][train], df['X'][test], df['Y'][train], df['Y'][test]\n",
    "    \n",
    "    tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                     token_pattern=r'\\w{1,}', ngram_range=(1, 3),\n",
    "                     use_idf=1, smooth_idf=1,\n",
    "                     sublinear_tf=1, stop_words='english')\n",
    "    \n",
    "    # gen features\n",
    "    X_train = [r for r in X_train.apply(lambda sentence: process_features(sentence))]\n",
    "    X_train = tfv.fit_transform(X_train)\n",
    "    y_train = list(y_train)\n",
    "    classifier = LogisticRegression()\n",
    "#     classifier = GradientBoostingClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    X_test = [r for r in X_test.apply(lambda sentence: process_features(sentence))]\n",
    "    X_test = tfv.transform(X_test).toarray()\n",
    "    y_test = list(y_test)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    \n",
    "    print(gen_scores(y_test, y_pred, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc': 0.99038705210473443, 'precision': 0.97776012708498805, 'recall': 0.87181303116147313, 'f1': 0.92175215275177835}\n",
      "{'roc': 0.98760305315662478, 'precision': 0.97676282051282048, 'recall': 0.87258410880458126, 'f1': 0.92173913043478262}\n",
      "{'roc': 0.99275668832025299, 'precision': 0.9699918896999189, 'recall': 0.87235594456601018, 'f1': 0.91858678955453155}\n",
      "{'roc': 0.99268153038229545, 'precision': 0.97751937984496129, 'recall': 0.8708563535911602, 'f1': 0.92111029948867795}\n",
      "{'roc': 0.99396155907785877, 'precision': 0.9761354888375674, 'recall': 0.88733379986004202, 'f1': 0.92961876832844581}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "def gen_scores(y_true, y_pred, y_pred_proba):\n",
    "    roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return { 'roc': roc, 'precision': precision, 'recall': recall, 'f1': f1 }\n",
    "   \n",
    "def process_features(sentence):\n",
    "#     return re.sub(r'\\d+', '0', sentence)\n",
    "    return sentence\n",
    "#     ret = \"\"\n",
    "    \n",
    "#     for w in sentence:\n",
    "#         if w.isdigit() and int(w) < 110:\n",
    "#             ret += '0'\n",
    "#         else:\n",
    "#             ret += w\n",
    "            \n",
    "#     return ret\n",
    "    \n",
    "size = 30\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train, test in kf.split(df):\n",
    "    X_train, X_test, y_train, y_test = df['X'][train], df['X'][test], df['list'][train], df['list'][test]\n",
    "    \n",
    "    tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                     token_pattern=r'\\w{1,}', ngram_range=(1, 3),\n",
    "                     use_idf=1, smooth_idf=1,\n",
    "                     sublinear_tf=1, stop_words='english')\n",
    "    \n",
    "    # gen features\n",
    "    X_train = [r for r in X_train.apply(lambda sentence: process_features(sentence))]\n",
    "    X_train = tfv.fit_transform(X_train)\n",
    "    y_train = list(y_train)\n",
    "    classifier = LogisticRegression()\n",
    "#     classifier = GradientBoostingClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    X_test = [r for r in X_test.apply(lambda sentence: process_features(sentence))]\n",
    "    X_test = tfv.transform(X_test).toarray()\n",
    "    y_test = list(y_test)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    \n",
    "    print(gen_scores(y_test, y_pred, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabeledSentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-359-518b4347b3c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLabeledSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-359-518b4347b3c6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLabeledSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabeledSentence' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def gen_sentence_features(w2v, sentence):\n",
    "    features = np.array([0.0]*size)\n",
    "    for w in sentence:\n",
    "        if w in w2v:\n",
    "             features += w2v[w]\n",
    "    \n",
    "    return features/len(sentence.split(' '))\n",
    "\n",
    "\n",
    "def gen_scores(y_true, y_pred, y_pred_proba):\n",
    "    roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return { 'roc': roc, 'precision': precision, 'recall': recall, 'f1': f1 }\n",
    "   \n",
    "def process_features(sentence):\n",
    "#     return re.sub(r'\\d+', '0', sentence)\n",
    "    return sentence\n",
    "#     ret = \"\"\n",
    "    \n",
    "#     for w in sentence:\n",
    "#         if w.isdigit() and int(w) < 110:\n",
    "#             ret += '0'\n",
    "#         else:\n",
    "#             ret += w\n",
    "            \n",
    "#     return ret\n",
    "    \n",
    "size = 30\n",
    "kf = KFold(n_splits=2, shuffle=True)\n",
    "\n",
    "index = 0\n",
    "def get_index():\n",
    "    index += 1\n",
    "    return index - 1\n",
    "\n",
    "for train, test in kf.split(df):\n",
    "    X_train, X_test, y_train, y_test = df['X'][train], df['X'][test], df['Y'][train], df['Y'][test]\n",
    "    \n",
    "    X_train = [LabeledSentence(words=r.split(' '), labels = [get_index()]) for r in X_train]\n",
    "    model = gensim.models.Doc2Vec(X_train, size=size)\n",
    "    w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "    \n",
    "    # gen features\n",
    "    X_train = [r for r in X_train.apply(lambda sentence: gen_sentence_features(w2v, process_features(sentence)))]\n",
    "    y_train = list(y_train)\n",
    "    classifier = LogisticRegression()\n",
    "#     classifier = GradientBoostingClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    X_test = [r for r in X_test.apply(lambda sentence: gen_sentence_features(w2v, process_features(sentence)))]\n",
    "    y_test = list(y_test)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    \n",
    "    print(gen_scores(y_test, y_pred, y_pred_proba))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 ab'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'roc': 0.81284205049237268, 'precision': 0.7266265060240964, 'recall': 0.75718769617074699, 'f1': 0.7415923762680604}\n",
    "{'roc': 0.81494818875165942, 'precision': 0.73140742509251522, 'recall': 0.76263380632312672, 'f1': 0.7466942904149656}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.62771392, -0.37692767, -0.05795803, -0.51460409,  0.56070054]),\n",
       " array([ 0.5270079 , -0.18355932, -0.14096526, -0.44264327,  0.32411522])]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [r for r in X_train[0:2]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "# X = np.array([np.array([1, 2]), np.array([3, 4])])\n",
    "y = [1, 2]\n",
    "X\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                     token_pattern=r'\\w{1,}', ngram_range=(1, 3),\n",
    "                     use_idf=1, smooth_idf=1,\n",
    "                     sublinear_tf=1, stop_words='english')\n",
    "    \n",
    "# gen features\n",
    "X = [r for r in df['X'].apply(lambda sentence: process_features(sentence))]\n",
    "X = tfv.fit_transform(X)\n",
    "y_clickbait = list(df['Y'])\n",
    "y_listicle = list(df['list'])\n",
    "classifier_clickbait = LogisticRegression()\n",
    "classifier_listicle = LogisticRegression()\n",
    "\n",
    "#     classifier = GradientBoostingClassifier()\n",
    "classifier_clickbait.fit(X, y_clickbait)\n",
    "classifier_listicle.fit(X, y_listicle)\n",
    "\n",
    "with open('classifier.p', 'wb') as f:\n",
    "    pickle.dump((classifier_clickbait, classifier_listicle, tfv), f)\n",
    "\n",
    "# print(gen_scores(y_test, y_pred, y_pred_proba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
